\chapter{IBM BigFix on SaaS, l'implementazione del prototipo}
L'attenta progettazione guidata dall'architect che abbiamo visto nel capitolo precedente è servita da base per la vera realizzazione del prototipo che rappresenta l'aspetto implementativo del progetto BigFix SaaS. Ma queste due fasi del lavoro che stiamo presentando non sono l'una successiva l'altra. E' bene rimarcare che, seguendo un approccio agile, iterativo e incrementale, l'intero disegno del progetto si è ottenuto solamente a fronte di una continua esplorazione dei requisiti che si è protratta durante tutta la fase implementativa. Il metronomo dello sviluppo sono state le sprint demo, le quali vedevano la partecipazione anche di altri stackeholders interni all'azienda, oltre che al team stesso. Da questi confronti sono risultati continui feedback che hanno contribuito a portare il lavoro al suo stato attuale.

\section{Container e microservizi di BigFix}
Per la necessità di continui feedback, nei primi sprint del progetto si è scelto di adottare una soluzione intermedia per quanto riguarda la scomposizione del prodotto nei container. Si deciso infatti di fare in modo che, in questa prima fase, ogni container ospitasse un relay o un server di uno dei clienti forniti dal SaaS, e non un microservizio vero e proprio del prodotto. Questa scelta è avvenuta dopo un'analisi dei requisiti. Scomporre BigFix in microservizi è un'operazione che richiede molte ore uomo di lavoro e affrontarla all'inizio del progetto non avrebbe consentito ti testare nelle primissime fasi le tecnologie che si erano individuate per il deployment, ossia l'aspetto più importante. Avendo presto a disposizione gli ambienti cliente con relay e server invece, si sono subito testati scenari tipici del prodotto on premises nelle nuove tecnologie SaaS adottate.

\section{Gli ambienti di sviluppo }
La realizzazione di un progetto aziendale richiede più ambienti nei quali, sequenzialmente, viene testato il prodotto prima di metterlo sul mercato. L'ambiente ufficiale finale, quello di produzione, deve garantire tutti i parametri qualitativi di cui abbiamo parlato precedentemente. Per questo motivo è lì che si concentrano i maggiori investimenti a livello hardware per garantire alte prestazioni di servizio. Al tempo stesso, però, occorre avere a disposizione un "laboratorio" in cui sviluppare e testare il software, in cui implementare le nuove features quando il prodotto sarà già sul mercato, un ambiente nel quale le modifiche che si compiono al codice non impattano in nessun modo il servizio offerto ai clienti. Per questo motivo il primo contesto in cui si è andato a lavorare è l'ambiente di sviluppo. Un ambiente chiuso all'esterno dell'azienda, ma che al tempo stesso simulasse nella maniera più fedele possibile l'ambiente di produzione. A questi due si aggiunge un'ambiente intermedio di pre-production, nel quale vengono svolti tutti i test qualitativi (QA) per verificare che il prodotto sia conforme ai parametri non funzionali stabiliti. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\textwidth,keepaspectratio=true]{capitoli/imgs/ambientisviluppo.png}
	\caption{Ambienti di BigFix SaaS}
\end{figure}

\section{Costruzione dell'ambiente e installazione di Docker e Kubernetes}
Vediamo una schematizzazione dell'ambiente di sviluppo nella figura \ref{ambs}. Kubernetes è il tool che muove le danze avendo la funzione di orchestratore dei container. L'architettura di questo framework richiede che ci sia un nodo master che coordina i nodi denominati kube-node, che possono anche rappresentare macchine diverse. Su tutti i kube-node è installato anche docker, in quanto ci permetterà di ospitare su di essi i container. Questi vengono deployati su tutti i nodi a disposizione seguendo l'algoritmo di scheduling round-robin in modo da gestire al meglio le risorse di calcolo. 
\paragraph{}
Il Kubernetes master si interfaccia, come posiamo vedere, anche con altre due macchine presenti nella rete: La macchina DB2 e la macchina NFS. Andiamo a vedere di cosa si tratta.
\paragraph{DB2}
E' la macchina che ospita una replica del database. Vedremo in seguito come, nel processo di onboarding di un nuovo cliente, vengono dinamicamente allocate le risorse del database al nuovo utente. Le modalità di interazione tra i componenti BigFix nei container e il database ricalcano in sostanza quelle del prodotto on premise.
\paragraph{Network File System (NFS)}
Questo componente è pensato per fornire storage per i container tramite protocollo NFS. L'NFS è un protocollo di rete sviluppato dalla Sun negli anni 80. Esso rappresenta un file system distribuito che consente ai computer di utilizzare la rete per accedere ai dischi rigidi remoti come fossero dischi locali. La componente NFS è stata utilizzata solamente in una prima fase del progetto in quanto, con il passaggio all'ambiente ufficiale, si è fatto affidamento allo storage fornito da un particolare componente di Kubernetes, il Service. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth,keepaspectratio=true]{capitoli/imgs/EnvironmentsComponentDiagram.png}
	\caption{L'ambiente di sviluppo di BigFix SaaS}
	\label{ambs}
\end{figure}


\section{Re-working degli installer del server e del relay di BigFix}
Il server e il relay di BigFix hanno i loro tradizionali installer relativi alla versione on premise del prodotto. Ovviamente è stato necessario un grosso lavoro di re-working di questi componenti per fare in modo che risultassero molto più leggeri e adatti al deployment in un container.

\section{Le immagini Docker}
Docker consente di deployare i container partendo da immagini di una repository fornita da Docker stesso. Nel nostro caso si è scelto di porre come base di tutti i container del servizio una macchina Linux CentOS 7 sia per i relay che per i server, una scelta dettata da esigenze aziendali. Docker fornisce delle versioni a container di molti sistemi operativi in modo da personalizzare secondo le proprie esigenze i servizi che si vanno ad implementare.

\paragraph{}
Questa macchina CentOS è ovviamente una versione molto embrionale dei servizi necessari per il prodotto. E' stato necessario creare delle directory di BigFix apposite per ospitare l'installer del server o del relay, a seconda dei casi. Si sono poste poi, all'interno di opportune cartelle, due file Bash fondamentali per il deployment: install.sh e start.sh. Nella prossima sezione spieghiamo il loro compito.

\section{Docker containers, Pods e services}
I file Bash di cui abbiamo parlato nella sezione precedente vengono in realtà eseguiti in contesti differenti. Install.sh viene chiamato in causa da Docker quando installa appunto il container. Successivamente Kubernetes chiama start.sh e avvia a tutti gli effetti i servizi di server e relay.
\paragraph{}
Ora occorre porre l'accento sulla modalità in cui i container interagiscono con Kubernetes. La scelta in questo progetto è stata quella di avere una corrispondenza biunivoca tra i container di Docker e i Pod di Kubernetes in modo da avere una semplicità di gestione dei container stessi. Vi è inoltre un ulteriore livello di astrazione sopra i Pod. Kubernetes offre infatti la possibilità di utilizzare il componente Service, che raggruppa in se uno o più pod. Il Service è stato utilizzato con il fine di poter utilizzare il pod nella rete in maniera molto più efficiente.
\paragraph{YAML} 
I Pod di Kubernetes utilizzano un particolare linguaggio di serializzazione, lo YAML (YAML Ain't Markup Language). Esso è per certi versi molto simile all'XML e al JSON, ma è molto più essenziale e quasi tutto il suo contenuto rappresenta il dato da serializzare. I suoi punti di forza sono la manutenibilità e la flessibilità. Un file JSON infatti è anche un file YAML valido, esso va a rappresentare quindi un sottoinsieme degli YAML possibili. All'interno dei file YAML si sono configurati tutti gli aspetti dei Pod che sono necessari a Kubernetes per far partire i servizi relay e server con la con i settaggi desiderati. Vediamo nella figura \ref{fig:yaml} un esempio di configurazione per un Pod.
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{capitoli/imgs/yaml}
	\caption{Esempio di YAML di configurazione per Kubernetes}
	\label{fig:yaml}
\end{figure}


\section{Modifiche al relay necessarie per la versione SaaS}
Il componente relay ha avuto bisogno di numerosi accorgimenti per permettere il suo funzionamento in prospettiva SaaS. Per sua natura, ha bisogno di numerose informazioni a riguardo del server al quale fa riferimento. Per configurare il relay correttamente è necessario far rinvenire ad esso alcuni file contenenti informazioni di riferimento al server. Questa era una procedura che nella versione on premise veniva fatta a mano. Ora, nella realizzazione del servizio in SaaS, si è implementato un procedimento automatico che fa affidamento sullo storage condiviso fornito dall'NFS.
\paragraph{Relay scaling}
Altra proprietà del relay è la capacità di scalare adeguatamente rispondendo alle necessità dinamiche dei clienti. Durante il tipico utilizzo del prodotto è molto comune che il numero dei client o la mole di lavoro richiesta cambino dinamicamente. Superata una certa soglia di workload o un certo numero di client (circa 1000), un singolo relay non è più in grado di far fronte alle richieste, ma ha bisogno che parte dell'onere di lavoro venga presa in carico da una sua copia identica. Questo processo viene chiamato di "scale up". Analogamente possono verificarsi delle situazioni in cui le risorse allocate siano sovradimensionate rispetto al carico di lavoro. Fortunatamente Kubernetes fornisce un comando ad hoc per entrambe queste necessità. Questo strumento utilizza dei volumi condivisi di appoggio per far sì che le nuove copie dei relay non si debbano configurare da capo. Il nuovo infatti eredita dal primo relay sia tutte le configurazioni di BigFix e di Docker, ma anche la cache dell'altro relay, in maniera tale da essere subito operativo in pochissimi secondi. 
\paragraph{Air-gap}
Un'altro elemento che è necessario per abbattere i tempi di funzionamento è l'Air-gap. Senza scendere il dettagli e tecnicismi, questo componente ci consente di scaricare Fixlet e altre configurazioni proprie di BigFix sul nuovo relay da un repository per i clienti (BES Support) in tempi estremamente rapidi. 
\section{Automazione del Deployment}
Facciamo il punto della situazione cercando di individuare gli elementi che sono stati definiti fino a questo punto. Abbiamo da un lato la possibilità di deployare dei container che svolgano le funzioni di server o di relay perfettamente funzionanti, da un'altro c'è il database DB2 pronto a fornire persistenza a tutti i dati del cliente, da un'altro ancora altri componenti necessari all'ambiente per il corretto funzionamento, come l'NFS. Supponiamo che in un istante, non predicibile ovviamente, un nuovo cliente acquisti la licenza ad utilizzare BigFix SaaS e abbia appena compilato la form con tutti i dati necessari. Cosa manca? 
\paragraph{}
Non è certo pensabile che a questo punto ci sia bisogno di un intervento umano per mettere a disposizione del nuovo cliente tutte le risorse necessarie. Ci si aspetta che il servizio sia disponibile nel giro di pochi minuti e la richiesta può essere avvenuta da qualunque parte del mondo e in qualunque momento della giornata.
\paragraph{}
Ecco quindi che è necessario un processo di software automation. Un processo automatico cioè che porti il prodotto ad essere pronto all'utilizzo da parte del nuovo cliente senza che gli sviluppatori o il team di operation intervengano. Questo processo deve essere tollerante a malfunzionamenti e deve poter interagire con tutte le componenti facenti parte del sistema. Si comporta, in sostanza, come una figura umana dedita all'istallazione. 

\subsection{Bash Scripting}
Nel progetto si è fatto largo uso del linguaggio Bash. Dovendo interagire con sistemi UNIX sono stati spesso inseriti file Bash in processi di automazione presenti negli scenari di gestione del sistema.
\paragraph{}
Bash, acronimo che sta per Bourne-Again-Shell, è un'interfaccia a riga di comando pensata per gestire i sistemi operativi UNIX nata alla fine degli anni '80. Come altri strumenti analoghi, oltre alla modalità interattiva, prevede anche la possibilità di creare script, con funzioni, cicli e costrutti tipici, e di eseguirli poi in blocco. Questi script sono contraddistinti dall'estensione .sh e dall'incipit "\#!/bin/bash", il quale indica il percorso della shell che dovrà eseguire lo script, Bash per l'appunto. 
\paragraph{Utilizzo degli script Bash nel progetto}
Gli script Bash sono stati di fondamentale importanza per il progetto. Data la loro versatilità e potenza è stato possibile utilizzarli per molti scopi di configurazione. Sono entrati, ad esempio, in flussi Ansible e Jenkins. Questi script venivano mandati tramite scp su opportune macchine e, una volta configurati i permessi, venivano eseguiti per svolgere i più disparati compiti necessari.
\subsection{Jenkins}
Il solo utilizzo di script bash non può garantire la corretta gestione di un'architettura così complessa come quella del SaaS di BigFix. Gli step di configurazione necessari ogni qualvolta si debba effettuare l'onboarding di un nuovo cliente sono numerosi e ripetitivi e quindi risulta molto utile adoperare un tool di software automation.
\paragraph{}
Per questo motivo si è scelto di studiare l'integrazione di uno dei più diffusi strumenti di continuous integration: Jenkins. Esso è un progetto open-source, distaccatosi da Oracle, nato negli ultimi anni. Jenkins ci consente di eseguire e monitorare sistematicamente task ripetitivi come possono essere ad esempio i processi di building o il deployment dei container. Il sistema, disponibile tramite server web, ha la possibilità di essere integrato con innumerevoli plugin che lo rendono adatto a automatizzare i task più disparati sulle macchine più eterogenee. 
\paragraph{}
Nel lavoro di tesi si sono utilizzati proprio alcuni di questi plugin per permettere, a partire dalla macchina di Automation Manager in cui è stato installato Jenkins, di orchestrare tutti i task sulle singole macchine dell'ambiente tramite ssh e scp.
\paragraph{}
Uno step preliminare che è stato necessario è quello di distribuire opportuni certificati alle macchine in gioco, in modo da poter consentire la comunicazione tramite Jenkins. In questo modo non era più necessario esplicitare ogni volta la chiave ssh, ma si instaura una comunicazione a chiave pubblica e chiave privata. 
\paragraph{}
A questo punto è necessario scrivere opportuni script Bash per i diversi task che sono necessari per allocare le opportune risorse a ogni cliente. I tipici step necessari nelle diverse macchine del nostro ambiente sono i seguenti:
\begin{itemize}
	\item Creazione di opportune directory nell'NFS per ospitare file di configurazione.
	\item Validazione della licenza di utilizzo del servizio 
	\item Creazione e connessione al database per i dati dell'utente e configurazione dello stesso.
	\item Personalizzazione degli YAML relativi al server e al relay
	\item Creazione e avvio dei servizi del server
	\item Creazione e avvio dei servizi dei relay
\end{itemize}
\paragraph{}
Ovviamente per eseguire questi passaggi è necessario interagire con tutte le macchine dell'ambiente. Jenkins ha la responsabilità di guidare questi passaggi e prendere opportune contromisure a eventuali malfunzionamenti. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/jenkisProj}
	\caption{Una delle schermate di configurazione di Jenkins}
	\label{fig:jenkisproj}
\end{figure}

\subsection{Ansible}
\paragraph{}
Dopo un confronto con il security operations team però, si è deciso di sostituire l'utilizzo di Jenkins con l'adozione di un utilizzo combinato di Ansible e IBM UrbanCode Deploy. Il security operation è un team di IBM, situato in Irlanda, che avrà il compito di porsi tra i clienti BigFix SaaS e il team di sviluppo quando il sistema è in produzione. Tra i suoi compiti ci sono quello di monitorare le attività del SaaS, di porre rimedi a eventuali miglioramenti, di intervenire in caso di cali di prestazioni e molto altro. Dal confronto tra questi due team si è evinta la necessità di allineare i tool di BigFix SaaS a quelli degli altri prodotti cloud di IBM, in particolar modo UrbanCode. Un'altra motivazione che ha spinto la scelta verso UrbanCode è la possibilità di fare code promotion tra più ambienti di sviluppo, come vedremo in seguito. 
\paragraph{}
Relativamente alle modalità di utilizzo che si sono fatte in questo progetto Ansible ha sempre rappresentato un'alternativa all'utilizzo di Jenkins. Ansible è uno strumento di automation nato nel 2012. Il tool si basa su due tipi di componenti, le macchine controllori che fanno partire l'automation e i nodi che sono le macchine dove avvengono eseguiti i task. 
\paragraph{}
Nel progetto è stata sfruttata molto la modularità di Ansible che permettere di sfruttare le strutture dei playbook e dei roles. Sostanzialmente un role è una successione di determinati task che hanno un fine e un target comune. I playbook sono delle collezioni di roles, i quali vengono così assemblati a piacimento per ottenere le configurazioni desiderate, come nel caso dell'ambiente di BigFix SaaS. 
\subsection{UrbanCode Deploy e code promotion}
C'è ancora un tool che è necessario integrare per garantire continuous integration e continuous delivery. Teniamo per un attimo in mente gli ambienti di sviluppo di cui abbiamo parlato nella sezione 7.2. Sarebbe molto utile che, ogni qualvolta viene effettuata una modifica al codice, un processo automatico integri questa modifica sequenzialmente negli ambienti di development, pre-production e production. Il passaggio tra questi ambienti può essere automatizzato ponendo come condizione di promozione all'ambiente successivo opportuni test automatici. Questo processo è chiamato di code promotion. 
\paragraph{}
Il tool in questione è un prodotto di IBM chiamato UrbanCode Deploy. Esso consente di coordinare e automatizzare le implementazioni delle applicazioni, le configurazioni di middleware e le modifiche ai database in ambienti di produzione e di test on-premise o basati su cloud. Nel progetto di tesi UrbanCode monitora l'esito dei playbook di Ansible e, in base al loro esito, decreta se fare o meno code promotion.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/ucd}
	\caption{IBM UrbanCode Deploy}
	\label{fig:ucd}
\end{figure}

\section{Scenario di onboarding di un nuovo cliente}
Questo scenario è stato il uno di quelli che ha richiesto più attenzione e quindi affrontato tra i primissimi. Dato il coinvolgimento di molte componenti del sistema è stato uno dei primissimi ad essere realizzato e testato. 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/onbordingScenarioScheda}
	\caption{Scheda dello scenario di onboarding}
	\label{fig:onbordingscenarioscheda}
\end{figure}
\paragraph{}
L'utente, come primo step, riempie una form come quella nella figura \ref{fig:saasform} con tutti i dati necessari al sistema per configurare correttamente il suo ambiente. Tra questi dati necessario troviamo anche l'indirizzo email in modo da notificare la disponibilità del servizio al termine del processo. 
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/saasForm}
	\caption{Form per il cliente di BigFix SaaS}
	\label{fig:saasform}
\end{figure}
\paragraph{}
A questo punto entra in gioco l'automation di UrbanCode e Ansible che prende in pasto i dati appena inseriti nella form. Come prima cosa si mette in piedi il database per l'utente corrente. Questa operazione richiede un discreto numero di configurazioni, che come anche le successive, vengono effettuate grazie a file bash opportunamente richiamati da Ansible. 
\paragraph{}
Una volta fatto partire il database, occorre "costruire" i file di YAML necessari per creare i Pod di Kubernetes. Questi file vengono scritti partendo da un template che presenta dei placeholder. Loro indicano sia i dati dell'utente corrente, sia le versioni di build del prodotto stesso. Il patching di questi file avviene tramite il linguaggio sed (stream editor), utilizzato per applicare espressioni regolari.
\paragraph{}
Gli YAML che ora sono personalizzati con i dati e le configurazioni del nuovo utente possono ora essere finalmente dati in pasto a Kubernetes, il quale installa i container di Docker e avvia i servizi del server e del relay, come vediamo in figura. \ref{fig:kubedashboard}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/kubedashboard}
	\caption{Dashboard di Kubernetes}
	\label{fig:kubedashboard}
\end{figure}
\paragraph{}
Gli ultimi step da compiere sono quelli fornire un IP pubblico a questi servizi, in modo da renderli accessibili al cliente, e mandare una mail al cliente fornendo un comodo link per accedere ai servizi della suite BigFix SaaS. Come possiamo notare, dalla prospettiva dell'esperienza utente, è richiesto solamente che il cliente compili la form iniziale. Dopo una breve attesa i servizi sono pronti all'utilizzo direttamente dal dispositivo dal quale è stata compiuta la richiesta. Questo scenario può rappresentare una vera rivoluzione confrontandolo con l'analogo caso d'uso nella versione on premise di BigFix.

\section{Il passaggio all'ambiente di produzione}
La caratteristica principale di un ambiente di sviluppo è che esso non è raggiungibile dall'esterno. La comunicazione si sviluppa solamente tra macchine della rete interna dell'azienda al fine di preservare l'ambiente ufficiale dalle modifiche al prodotto prima che queste vengano completamente testate e collaudate. Nel passaggio all'ambiente di produzione, devo sono state effettuate anche diverse sprint demo nel corso del progetto, si deve tener conto di alcuni accorgimenti da apporre al prodotto. Occorre infatti che i container di BigFix SaaS possiedano un IP pubblico per essere raggiunte dalla rete. Per fare ciò si è utilizzato il componente "Service" di Kubernetes che, wrappando il pod, gli fornisce un IP accessibile via rete e quindi, comunicabile immediatamente all'utente al termine del processo di onboarding.

\section{Automazione del Testing}
Quella del test automation è un'attività molto importante nel contesto di un progetto SaaS. Non si può pensare infatti di fare Continuous Integration delle frequenti modifiche che si apportano ai microservizi, effettuando un collaudo manuale del software. Occorre che un processo automatico si sostituisca all'uomo e effettui test approfonditi sia dal punto di vista sia funzionale che qualitativo, al termine dei quali si prendono provvedimenti o si portano le modifiche in produzione in modo che siano fruibili da tutti gli utenti. Il test non fa altro che comparare i risultati ottenuti dalle sue simulazioni con i risultati attesi dal prodotto. Al termine di ogni test è necessario che venga prodotto un report che possa essere consultabile dai team di operations. Alcuni progetti danno tanta importanza al test automation da sviluppare il realtà prima i test e poi il prodotto stesso. Si sviluppa poi guidati dall'obiettivo di far superare tali test. Questa tecnica è chiamata viluppo guidato dal test (TDD, Test Driven Development).
\paragraph{Continuous Delivery}
In questo progetto si vuole che le modifiche che il team apporta al codice attraversino una serie di controlli prima di giungere alla totalità dei clienti. Le nuove modifiche infatti, entrano in un processo automatico composto di numerosi test. Dapprima vengono effettuati dei test funzionali, per appurare che le nuove modifiche non abbiano intaccato le funzionalità del prodotto. Poi il nuovo codice giunge in un ambiente di Quality Assurance (QA) dove, sempre automaticamente, vengono effettuati test di performance e di security a cascata. Se i nuovi componenti passano tutti i test previsti dal flusso automatico giungono finalmente in ambiente di produzione, dove sono fruibili dai clienti. 
\paragraph{}
Anche questo ultimo passaggio, però, non è immediato. Sostituendo simultaneamente tutte le repliche del microservizio aggiornato si incorrerebbe in una non availability dell'intero sistema. Ciò non è accettabile in un servizio SaaS. Occorre aggiornare i container quindi un po' alla volta, redirigendo tramite il Load Balancer opportunamente il traffico su quei container ancora attivi o già aggiornati e riavviati. L'esperienza utente in questo modo non risentirà di nessun disservizio.
\subsection{Functional Test}
Come detto poc'anzi, i primi test in cui un nuovo microservizio deve fare i conti sono quelli funzionali. Presso IBM Security si sono canonizzati una collezione di test cases che verificano peculiarmente tutte le specifiche di BigFix nella sua versione onpremise. Questi test vengono svolti mediante un tool sviluppato nel team in cui ho lavorato che si basa su JUnit, un framework Java per compiere unit testing.
\subsubsection{JUnit}
JUnit è il più diffuso framework Java per compiere unit testing. Lo unit test rappresenta l'attività di testare singoli moduli software compiuta direttamente dagli stessi sviluppatori per testare singole porzioni di codice appena implementate. Lo scopo è quello di individuare eventuali bug da correggere prima di integrare le modifiche. JUnit consente di estendere le classi che intendiamo testare e tramite delle annotations indichiamo quali sono i metodi da invocare nei test cases.
\subsubsection{JUTAA}
JUTAA (Java Unified Test Automation Architecture) estende proprio JUnit. Presso IBM si è realizzato questo tool per migliorare la qualità dei test effettuati su BigFix e aumentarne il riuso. L'architettura è relativamente semplice. Una macchina funge da JUTAA master è può interagire sia con i server di BigFix che con i relay in modo da far girare su di essi i test case. Questi sono opportunamente raggruppati e catalogati. Per comunicare con le macchine slave JUTAA utilizza un'altro componente IBM, denominato STAF (Software Testing Automation Framework). Vediamo nella figura \ref{fig:jutaaarchitecture} una schematizzazione dell'architettura di JUTAA.
\paragraph{}
E' stato ovviamente necessario un processo di adattamento dei test cases in versione on premise alla versione SaaS. Questo lavoro ha visto l'estensione di classi Java precedentemente scritte per la versione on premise aggiungendo o sovrascrivendo opportuni metodi. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/jutaaarchitecture}
	\caption{Architettura di JUTAA}
	\label{fig:jutaaarchitecture}
\end{figure}
\paragraph{Step per eseguire un test con JUTAA}
\begin{itemize}
	\item Test costruction \\
	JUTAA presenta una struttura modulare. E' possibile assemblare ad hoc i test di cui si ha bisogno tramite due file di configurazione in XML: env.xml e suite.xml. 
	\item Test execution\\
	Tramite un main.java viene lanciata la suite di test precedentemente dichiarata. Il framework scarica automaticamente il software più aggiornato da una repository.
	\item Test results analisys \\
	Tramite opportuni accorgimenti si raccolgono e si visualizzano i risultati dei test, determinando eventuali malfunzionamenti.
\end{itemize}
\subsection{Pre-production environment}
A questo punto il software è pronto per passare nell'ambiente di pre-produzione. Qui i servizi devono superare gli ultimi ostacoli, ossia i test qualitativi. In sequenza verranno effettuati test di security, di performance e di penetration al termine dei quali il prodotto entra in rolling release. Questo ambiente è curato dagli esperti di Quality Assurance che utilizzano metodi consolidati di testing come andremo a vedere nelle prossime sezioni.
\subsection{Security Test}
Una tematica di testing fondamentale per un servizio SaaS è quella di security. In questo ambito, per BigFix SaaS si compiono due tipi di indagini differenti. Una riguarda le exposure di sicurezza e una la riservatezza dei dati.
\subsubsection{Image Compliance}
Il concetto di base è che ogni componente o tecnologia che man mano si aggiunge al progetto ha delle vulnerabilità ben note. Il test si basa su una precisa descrizione e catalogazione di queste vulnerabilità. All'aggiunta di un nuovo componente software, Image compliance va a controllare se questo componente potrebbe esporre il prodotto a qualche vulnerabilità. Nel caso affermativo si va a controllare se quella vulnerabilità è stata adeguatamente coperta in modo da renderne il servizio immune. Questo compito è svolto da un tool di un progetto open source denominato Clair, che aiuta proprio in questo tipo di comunicazioni.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{capitoli/imgs/clair}
	\caption{Clair}
	\label{fig:clair}
\end{figure}
\subsubsection{AppScan}
AppScan è un prodotto di IBM che ha come finalità quella di monitorare la vulnerabilità dei dati sensibili. Nel caso specifico di BigFix, occorre porre molta attenzione a questa tematica in quanto i dati delle singole aziende clienti sono molto sensibili e la filosofia del multitenancy ci esporrebbe a notevoli rischi. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{capitoli/imgs/ibm-appscan-logo}
	\caption{IBM AppScan}
	\label{fig:ibm-appscan-logo}
\end{figure}

\subsection{Performance Test}
I test di performance devono stabilire se il prodotto è conforme a dei parametri che sono stati definiti in fase di progettazione. Per fare ciò si sono implementati in IBM Security dei simulatori che, mediante l'utilizzo di hardware molto potente, riesce a simulare l'utilizzo del prodotto con un carico di lavoro considerevole. Questi simulatori sfruttano diverse tecnologie per rendere la simulazione più fedele possibile ad un utilizzo reale.

\subsection{Penetration Test}
Il penetration test per il prodotto BigFix SaaS è ancora in fase di studio. Questi tipi di test sono quelli relativamente più complessi in quanto si basano sulla simulazione di attacchi da parte di utenti malintenzioanti. E' in corso una fase di studio nell'individuazione di tecniche e tool di automation di questi tipi di test che, per certi aspetti, possono essere i più ardui da automatizzare in quanto si basano su comportamenti umani meno prevedibili rispetto alle dinamiche delle altre tipologie di test.

\subsection{Rielaborazione degli output del testing}
Gli output di questa grande mole di test occorre che siano analizzati e interpretati approfonditamente per decretare se una nuova modifica del prodotto che si esegue nei continui cicli di produzione possa essere distribuita presso i clienti. Per questo è necessario sviluppare un componente che sia in grado di prendere decisioni riguardo al passaggio tra i diversi ambienti di produzione del prodotto e possa presentare i risultati rielaborati al personale incaricato.

\section{Scenario di upgrade del servizio}
Questo scenario vede come attori del sistema il team di DevOps. E' anche questo uno scenario emblematico del passaggio al SaaS in quanto mostra le peculiarità di un aggiornamento del prodotto, o per meglio dire una parte di esso, ovvero i microservizi. Lo scenario che vogliamo presentare riguarda l'integrazione di alcune nuove modifiche di un microservizio nel prodotto che è presente sul mercato. 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{capitoli/imgs/upgradescenarioScheda}
	\caption{Scheda dello scenario di onboarding}
	\label{fig:upgradescenarioscheda}
\end{figure}
\paragraph{}
La problematica principale da risolvere è quella di mantenere un'alta availability del sistema nonostante alcune sue componenti vengano aggiornate e sostituite. Prima di giungere ai clienti però, le nuove modifiche devono passare la lunga filiera dei test che abbiamo descritto nei paragrafi precedenti. Il processo, pur sotto la supervisione del team di DevOps, è completamente automatico. 
\paragraph{}
Quando è pronta una nuova build, viene triggerato un apposito processo di UrbanCode che, notando la presenza di questa build, cerca di passare le modifiche dall'ambiente di sviluppo a quello di pre-produzione e poi a quello di produzione entrambe dopo aver effettuati opportuni test. Come prima cosa viene effettuato un deploy dei microservizi che sono stati interessati dagli aggiornamenti in modo che su di essi si possano girare opportuni test. Una volta preparato l'ambiente, vengono chiamati in causa opportuni test case di JUTAA per verificare che i requisiti funzionali del prodotto vengano rispettati. Ovviamente è sottinteso che i singoli componenti implementati o modificati siano stati già opportunamente testati del team che li ha sviluppati. 
\paragraph{}
Se i risultati dei test funzionali sono soddisfacenti, il servizio con le nuove modifiche appena apportate passa e nell'ambiente di pre-production per i test di QA. Una volta effettuato tutto il ciclo di test non funzionali può iniziare la Rolling Release. A mano a mano vengono aggiornate tutte le repliche con la nuova versione di build, monitorando poi anche eventuali problematiche che possono sorgere con questa operazione. Il processo di Rolling Release continuerà,  finché tutti i clienti utilizzeranno solamente la versione aggiornata del prodotto.


